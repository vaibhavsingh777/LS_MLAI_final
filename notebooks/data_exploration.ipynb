{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_exploration.ipynb\n",
    "\n",
    "# 1. Imports\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2. Load datasets\n",
    "print(\"Loading raw dataset...\")\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "print(\"Loading filtered dataset...\")\n",
    "filtered_dataset = load_from_disk(\"../data/filtered_wikitext2\")\n",
    "print(\"Loading tokenized dataset...\")\n",
    "tokenized_dataset = load_from_disk(\"../data/tokenized_wikitext2\")\n",
    "\n",
    "# 3. Basic info\n",
    "print(\"Raw splits:\", raw_dataset)\n",
    "print(\"Filtered splits:\", filtered_dataset)\n",
    "print(\"Tokenized splits:\", tokenized_dataset)\n",
    "\n",
    "# 4. Show sample texts\n",
    "def show_sample(dataset, split, n=3):\n",
    "    print(f\"\\n--- {split.upper()} SAMPLES ---\")\n",
    "    for i in range(n):\n",
    "        print(f\"{i+1}: {dataset[split][i]['text']}\")\n",
    "\n",
    "show_sample(raw_dataset, 'train')\n",
    "show_sample(filtered_dataset, 'train')\n",
    "\n",
    "# 5. Text length distribution (filtered)\n",
    "lengths = [len(sample['text'].split()) for sample in filtered_dataset['train']]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Token count per sample (Filtered Train Split)\")\n",
    "plt.xlabel(\"Tokens per sample\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Tokenized sequence length distribution\n",
    "seq_lengths = [len(sample['input_ids']) for sample in tokenized_dataset['train']]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(seq_lengths, bins=30, color='salmon', edgecolor='black')\n",
    "plt.title(\"Input ID sequence length (Tokenized Train Split)\")\n",
    "plt.xlabel(\"Sequence length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 7. Vocabulary size\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"Vocabulary size (GPT-2):\", tokenizer.vocab_size)\n",
    "\n",
    "# 8. Show tokenized sample\n",
    "print(\"\\n--- Tokenized train sample ---\")\n",
    "sample = tokenized_dataset['train'][0]\n",
    "print(\"Input IDs:\", sample['input_ids'])\n",
    "print(\"Attention mask:\", sample['attention_mask'])\n",
    "print(\"Decoded text:\", tokenizer.decode(sample['input_ids']))\n",
    "\n",
    "# 9. Random qualitative samples\n",
    "print(\"\\n--- Random filtered samples ---\")\n",
    "for _ in range(3):\n",
    "    idx = random.randint(0, len(filtered_dataset['train']) - 1)\n",
    "    print(filtered_dataset['train'][idx]['text'])\n",
    "\n",
    "print(\"\\n--- Random tokenized samples (decoded) ---\")\n",
    "for _ in range(3):\n",
    "    idx = random.randint(0, len(tokenized_dataset['train']) - 1)\n",
    "    ids = tokenized_dataset['train'][idx]['input_ids']\n",
    "    print(tokenizer.decode(ids))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
